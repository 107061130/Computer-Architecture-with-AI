{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cb87008-47a1-4c2a-b04d-9931c14f1cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "from onnx import shape_inference\n",
    "from os import path\n",
    "import sys\n",
    "from tabulate import tabulate\n",
    "from onnx import onnx_ml_pb2 as xpb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "203202ef-c3e3-4912-8ae1-52d37d589ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape inference complete ...\n"
     ]
    }
   ],
   "source": [
    "model = onnx.load('./models/alexnet.onnx')\n",
    "onnx.checker.check_model(model)\n",
    "inferred_model = shape_inference.infer_shapes(model)\n",
    "print('shape inference complete ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8824e-094c-4483-b08e-ac792702082f",
   "metadata": {},
   "source": [
    "### model characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77a29415-21d8-4b1b-b772-ac8d79efe7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ONNX operators: 20\n",
      "Unique ONNX operator names:\n",
      "- AveragePool\n",
      "- Conv\n",
      "- Gemm\n",
      "- Flatten\n",
      "- MaxPool\n",
      "- Relu\n",
      "\n",
      "Attributes for each operator:\n",
      "\n",
      "Operator: Conv\n",
      "dilations: [1, 1]\n",
      "group: []\n",
      "kernel_shape: [11, 11]\n",
      "pads: [2, 2, 2, 2]\n",
      "strides: [4, 4]\n",
      "\n",
      "Operator: Relu\n",
      "\n",
      "Operator: MaxPool\n",
      "ceil_mode: []\n",
      "dilations: [1, 1]\n",
      "kernel_shape: [3, 3]\n",
      "pads: [0, 0, 0, 0]\n",
      "strides: [2, 2]\n",
      "\n",
      "Operator: Conv\n",
      "dilations: [1, 1]\n",
      "group: []\n",
      "kernel_shape: [5, 5]\n",
      "pads: [2, 2, 2, 2]\n",
      "strides: [1, 1]\n",
      "\n",
      "Operator: Relu\n",
      "\n",
      "Operator: MaxPool\n",
      "ceil_mode: []\n",
      "dilations: [1, 1]\n",
      "kernel_shape: [3, 3]\n",
      "pads: [0, 0, 0, 0]\n",
      "strides: [2, 2]\n",
      "\n",
      "Operator: Conv\n",
      "dilations: [1, 1]\n",
      "group: []\n",
      "kernel_shape: [3, 3]\n",
      "pads: [1, 1, 1, 1]\n",
      "strides: [1, 1]\n",
      "\n",
      "Operator: Relu\n",
      "\n",
      "Operator: Conv\n",
      "dilations: [1, 1]\n",
      "group: []\n",
      "kernel_shape: [3, 3]\n",
      "pads: [1, 1, 1, 1]\n",
      "strides: [1, 1]\n",
      "\n",
      "Operator: Relu\n",
      "\n",
      "Operator: Conv\n",
      "dilations: [1, 1]\n",
      "group: []\n",
      "kernel_shape: [3, 3]\n",
      "pads: [1, 1, 1, 1]\n",
      "strides: [1, 1]\n",
      "\n",
      "Operator: Relu\n",
      "\n",
      "Operator: MaxPool\n",
      "ceil_mode: []\n",
      "dilations: [1, 1]\n",
      "kernel_shape: [3, 3]\n",
      "pads: [0, 0, 0, 0]\n",
      "strides: [2, 2]\n",
      "\n",
      "Operator: AveragePool\n",
      "kernel_shape: [1, 1]\n",
      "strides: [1, 1]\n",
      "\n",
      "Operator: Flatten\n",
      "axis: []\n",
      "\n",
      "Operator: Gemm\n",
      "alpha: []\n",
      "beta: []\n",
      "transB: []\n",
      "\n",
      "Operator: Relu\n",
      "\n",
      "Operator: Gemm\n",
      "alpha: []\n",
      "beta: []\n",
      "transB: []\n",
      "\n",
      "Operator: Relu\n",
      "\n",
      "Operator: Gemm\n",
      "alpha: []\n",
      "beta: []\n",
      "transB: []\n"
     ]
    }
   ],
   "source": [
    "# Extract unique operator names\n",
    "# Print the number of ONNX operators\n",
    "num_operators = len(model.graph.node)\n",
    "print(f\"Number of ONNX operators: {num_operators}\")\n",
    "\n",
    "unique_operator_names = set(node.op_type for node in model.graph.node)\n",
    "print(\"Unique ONNX operator names:\")\n",
    "for op_name in unique_operator_names:\n",
    "    print(f\"- {op_name}\")\n",
    "\n",
    "# Print attributes for each operator\n",
    "print(\"\\nAttributes for each operator:\")\n",
    "for node in model.graph.node:\n",
    "    op_name = node.op_type\n",
    "    print(f\"\\nOperator: {op_name}\")\n",
    "\n",
    "    for attr in node.attribute:\n",
    "        print(f\"{attr.name}: {attr.ints}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6097eae7-69d7-43f2-809c-35d95e9a361f",
   "metadata": {},
   "source": [
    "### Data bandwidth requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "334a891a-0eaf-4600-a840-ddff231bb41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "layer                            read_bw    write_bw    total_bw\n",
      "-----------------------------  ---------  ----------  ----------\n",
      "/classifier/classifier.1/Gemm  151048192       16384   151064576\n",
      "/classifier/classifier.4/Gemm   67141632       16384    67158016\n",
      "/classifier/classifier.6/Gemm   16404384        4000    16408384\n",
      "/features/features.8/Conv        3799552      173056     3972608\n",
      "/features/features.6/Conv        2785536      259584     3045120\n",
      "/features/features.10/Conv       2533376      173056     2706432\n",
      "/features/features.3/Conv        1416192      559872     1976064\n",
      "/features/features.1/Relu         774400      774400     1548800\n",
      "/features/features.2/MaxPool      774400      186624      961024\n",
      "/features/features.0/Conv         695296      774400     1469696\n",
      "/features/features.4/Relu         559872      559872     1119744\n",
      "/features/features.5/MaxPool      559872      129792      689664\n",
      "/features/features.7/Relu         259584      259584      519168\n",
      "/features/features.9/Relu         173056      173056      346112\n",
      "/features/features.11/Relu        173056      173056      346112\n",
      "/features/features.12/MaxPool     173056       36864      209920\n",
      "/avgpool/AveragePool               36864       36864       73728\n",
      "/Flatten                           36864       36864       73728\n",
      "/classifier/classifier.2/Relu      16384       16384       32768\n",
      "/classifier/classifier.5/Relu      16384       16384       32768\n",
      "====================================================================================\n",
      "\n",
      "The memory bandwidth for processor to execute a whole model without on-chip-buffer is: \n",
      " 253754432 (bytes)\n",
      " 253.754432 (MB)\n",
      "\n",
      "op_name    unfound_tensor    op_type\n",
      "---------  ----------------  ---------\n",
      "====================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _parse_element(elem: xpb2.ValueInfoProto):\n",
    "    name = getattr(elem, 'name', \"None\")\n",
    "    data_type = \"NA\"\n",
    "    shape_str = \"NA\"\n",
    "    etype = getattr(elem, 'type', False)\n",
    "    if etype:\n",
    "        ttype = getattr(etype, 'tensor_type', False)\n",
    "        if ttype:\n",
    "            data_type = getattr(ttype, 'elem_type', 0)\n",
    "            shape = getattr(elem.type.tensor_type, \"shape\", False)\n",
    "            if shape:\n",
    "                shape_str = \"[\"\n",
    "                dims = getattr(shape, 'dim', [])\n",
    "                for dim in dims:\n",
    "                    vals = getattr(dim, 'dim_value', \"?\")\n",
    "                    shape_str += (str(vals) + \",\")\n",
    "                shape_str = shape_str.rstrip(\",\")\n",
    "                shape_str += \"]\"\n",
    "    return name, data_type, shape_str\n",
    "\n",
    "def get_valueproto_or_tensorproto_by_name(name: str, graph: xpb2.GraphProto):\n",
    "    for i, node in enumerate(inferred_model.graph.node):\n",
    "            if node.name == \"\":\n",
    "                inferred_model.graph.node[i].name = str(i)\n",
    "    input_nlist = [k.name for k in graph.input]\n",
    "    initializer_nlist = [k.name for k in graph.initializer]\n",
    "    value_info_nlist = [k.name for k in graph.value_info]\n",
    "    output_nlist = [k.name for k in graph.output]\n",
    "\n",
    "    # get tensor data\n",
    "    if name in input_nlist:\n",
    "        idx = input_nlist.index(name)\n",
    "        return graph.input[idx], int(1)\n",
    "    elif name in value_info_nlist:\n",
    "        idx = value_info_nlist.index(name)\n",
    "        return graph.value_info[idx], int(2)\n",
    "    elif name in initializer_nlist:\n",
    "        idx = initializer_nlist.index(name)\n",
    "        return graph.initializer[idx], int(3)\n",
    "    elif name in output_nlist:\n",
    "        idx = output_nlist.index(name)\n",
    "        return graph.output[idx], int(4)\n",
    "    else:\n",
    "        print(\"[ERROR MASSAGE] Can't find the tensor: \", name)\n",
    "        print('input_nlist:\\n', input_nlist)\n",
    "        print('===================')\n",
    "        print('value_info_nlist:\\n', value_info_nlist)\n",
    "        print('===================')\n",
    "        print('initializer_nlist:\\n', initializer_nlist)\n",
    "        print('===================')\n",
    "        print('output_nlist:\\n', output_nlist)\n",
    "        print('===================')\n",
    "        return False, 0\n",
    "\n",
    "def cal_tensor_mem_size(elem_type: str, shape: [int]):\n",
    "    \"\"\" given the element type of the tensor and its shape, and return its memory size.\n",
    "\n",
    "    Utility.\n",
    "\n",
    "    Args:\n",
    "        ttype: the type of the element of the given tensor. format: 'int', ...\n",
    "        shape: the shape of the given tensor. format: [] of int\n",
    "\n",
    "    Returns:\n",
    "        mem_size: int\n",
    "    \"\"\"\n",
    "    # init\n",
    "    mem_size = int(1)\n",
    "    # traverse the list to get the number of the elements\n",
    "    for num in shape:\n",
    "        mem_size *= num\n",
    "    # multiple the size of variable with the number of the elements\n",
    "    # \"FLOAT\": 1,\n",
    "    # \"UINT8\": 2,\n",
    "    # \"INT8\": 3,\n",
    "    # \"UINT16\": 4,\n",
    "    # \"INT16\": 5,\n",
    "    # \"INT32\": 6,\n",
    "    # \"INT64\": 7,\n",
    "    # # \"STRING\" : 8,\n",
    "    # \"BOOL\": 9,\n",
    "    # \"FLOAT16\": 10,\n",
    "    # \"DOUBLE\": 11,\n",
    "    # \"UINT32\": 12,\n",
    "    # \"UINT64\": 13,\n",
    "    # \"COMPLEX64\": 14,\n",
    "    # \"COMPLEX128\": 15\n",
    "    if elem_type == 1:\n",
    "        mem_size *= 4\n",
    "    elif elem_type == 2:\n",
    "        mem_size *= 1\n",
    "    elif elem_type == 3:\n",
    "        mem_size *= 1\n",
    "    elif elem_type == 4:\n",
    "        mem_size *= 2\n",
    "    elif elem_type == 5:\n",
    "        mem_size *= 2\n",
    "    elif elem_type == 6:\n",
    "        mem_size *= 4\n",
    "    elif elem_type == 7:\n",
    "        mem_size *= 8\n",
    "    elif elem_type == 9:\n",
    "        mem_size *= 1\n",
    "    elif elem_type == 10:\n",
    "        mem_size *= 2\n",
    "    elif elem_type == 11:\n",
    "        mem_size *= 8\n",
    "    elif elem_type == 12:\n",
    "        mem_size *= 4\n",
    "    elif elem_type == 13:\n",
    "        mem_size *= 8\n",
    "    elif elem_type == 14:\n",
    "        mem_size *= 8\n",
    "    elif elem_type == 15:\n",
    "        mem_size *= 16\n",
    "    else:\n",
    "        print(\"Undefined data type\")\n",
    "\n",
    "    return mem_size\n",
    "\n",
    "def get_bandwidth(graph: xpb2.GraphProto):\n",
    "    try:\n",
    "        mem_BW_list = []\n",
    "        total_mem_BW = 0\n",
    "        unknown_tensor_list = []\n",
    "        # traverse all the nodes\n",
    "        for nodeProto in graph.node:\n",
    "            # init variables\n",
    "            read_mem_BW_each_layer = 0\n",
    "            write_mem_BW_each_layer = 0\n",
    "            total_each_layer = 0\n",
    "            # traverse all input tensor\n",
    "            for input_name in nodeProto.input:\n",
    "                # get the TensorProto/ValueInfoProto by searching its name\n",
    "                proto, type_Num = get_valueproto_or_tensorproto_by_name(\n",
    "                    input_name, graph)\n",
    "                # parse the ValueInfoProto/TensorProto\n",
    "                if proto:\n",
    "                    if type_Num == 3:\n",
    "                        dtype = getattr(proto, 'data_type', False)\n",
    "                        # get the shape of the tensor\n",
    "                        shape = getattr(proto, 'dims', [])\n",
    "                    elif type_Num == 1 or type_Num == 2:\n",
    "                        name, dtype, shape_str = _parse_element(proto)\n",
    "                        shape_str = shape_str.strip('[]')\n",
    "                        shape_str = shape_str.split(',')\n",
    "                        shape = []\n",
    "                        for dim in shape_str:\n",
    "                            shape.append(int(dim))\n",
    "                    else:\n",
    "                        print(\n",
    "                            '[ERROR MASSAGE] [get_info/mem_BW_without_buf] The Tensor: ',\n",
    "                            input_name, ' is from a wrong list !')\n",
    "                else:\n",
    "                    print(\n",
    "                        '[ERROR MASSAGE] [get_info/mem_BW_without_buf] The Tensor: ',\n",
    "                        input_name, ' is no found !')\n",
    "                    unknown_tensor_list.append(\n",
    "                        (nodeProto.name, input_name, nodeProto.op_type))\n",
    "                # calculate the tensor size in btye\n",
    "                \n",
    "                read_mem_BW_each_layer += cal_tensor_mem_size(dtype, shape)\n",
    "\n",
    "            # traverse all output tensor\n",
    "            for output_name in nodeProto.output:\n",
    "                # get the TensorProto/ValueInfoProto by searching its name\n",
    "                proto, type_Num = get_valueproto_or_tensorproto_by_name(\n",
    "                    output_name, graph)\n",
    "                # parse the ValueInfoProto\n",
    "                if proto:\n",
    "                    if type_Num == 2 or type_Num == 4:\n",
    "                        # name, dtype, shape = utils._parse_ValueInfoProto(proto)\n",
    "                        name, dtype, shape_str = _parse_element(proto)\n",
    "                        shape_str = shape_str.strip('[]')\n",
    "                        shape_str = shape_str.split(',')\n",
    "                        shape = []\n",
    "                        for dim in shape_str:\n",
    "                            shape.append(int(dim))\n",
    "                    else:\n",
    "                        print(\n",
    "                            '[ERROR MASSAGE] [get_info/mem_BW_without_buf] The Tensor: ',\n",
    "                            output_name, ' is from a wrong list !')\n",
    "                else:\n",
    "                    print(\n",
    "                        '[ERROR MASSAGE] [get_info/mem_BW_without_buf] The Tensor: ',\n",
    "                        input_name, ' is no found !')\n",
    "                    unknown_tensor_list.append(\n",
    "                        (nodeProto.name, output_name, nodeProto.op_type))\n",
    "                # calculate the tensor size in btye\n",
    "                write_mem_BW_each_layer += cal_tensor_mem_size(dtype, shape)\n",
    "\n",
    "            # cal total bw\n",
    "            total_each_layer = read_mem_BW_each_layer + write_mem_BW_each_layer\n",
    "\n",
    "            # store into tuple\n",
    "            temp_tuple = (nodeProto.name, read_mem_BW_each_layer,\n",
    "                        write_mem_BW_each_layer, total_each_layer)\n",
    "            #append it\n",
    "            mem_BW_list.append(temp_tuple)\n",
    "            # accmulate the value\n",
    "            total_mem_BW += total_each_layer\n",
    "\n",
    "        # display the mem_bw of eahc layer\n",
    "        columns = ['layer', 'read_bw', 'write_bw', 'total_bw']\n",
    "        # resort the list\n",
    "        mem_BW_list = sorted(mem_BW_list,\n",
    "                             key=lambda Layer: Layer[1],\n",
    "                             reverse=True)\n",
    "        print(tabulate(mem_BW_list, headers=columns))\n",
    "        print(\n",
    "            '====================================================================================\\n'\n",
    "        )\n",
    "        # display it\n",
    "        print(\n",
    "            \"The memory bandwidth for processor to execute a whole model without on-chip-buffer is: \\n\",\n",
    "            total_mem_BW, '(bytes)\\n',\n",
    "            float(total_mem_BW) / float(1000000), '(MB)\\n')\n",
    "        # display the unknown tensor\n",
    "        columns = ['op_name', 'unfound_tensor', 'op_type']\n",
    "        print(tabulate(unknown_tensor_list, headers=columns))\n",
    "        print(\n",
    "            '====================================================================================\\n'\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"[ERROR MASSAGE] Unable to display: \" + str(e))\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "#從這裡開始\n",
    "print(\"start\")\n",
    "get_bandwidth(inferred_model.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6182330-85e1-4be5-ab0b-954289eb9423",
   "metadata": {},
   "source": [
    "### activation memory storage requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c477500-9c16-4922-8400-dc416859bccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'actual_input_1': [1, 3, 224, 224], activations = 150528\n",
      "'Conv_output_0': [1, 64, 55, 55], activations = 193600\n",
      "'Relu_output_0': [1, 64, 55, 55], activations = 193600\n",
      "'MaxPool_output_0': [1, 64, 27, 27], activations = 46656\n",
      "'Conv_output_0': [1, 192, 27, 27], activations = 139968\n",
      "'Relu_output_0': [1, 192, 27, 27], activations = 139968\n",
      "'MaxPool_output_0': [1, 192, 13, 13], activations = 32448\n",
      "'Conv_output_0': [1, 384, 13, 13], activations = 64896\n",
      "'Relu_output_0': [1, 384, 13, 13], activations = 64896\n",
      "'Conv_output_0': [1, 256, 13, 13], activations = 43264\n",
      "'Relu_output_0': [1, 256, 13, 13], activations = 43264\n",
      "'Conv_output_0': [1, 256, 13, 13], activations = 43264\n",
      "'Relu_output_0': [1, 256, 13, 13], activations = 43264\n",
      "'MaxPool_output_0': [1, 256, 6, 6], activations = 9216\n",
      "'AveragePool_output_0': [1, 256, 6, 6], activations = 9216\n",
      "'Flatten_output_0': [1, 9216], activations = 9216\n",
      "'Gemm_output_0': [1, 4096], activations = 4096\n",
      "'Relu_output_0': [1, 4096], activations = 4096\n",
      "'Gemm_output_0': [1, 4096], activations = 4096\n",
      "'Relu_output_0': [1, 4096], activations = 4096\n",
      "'output1': [1, 1000], activations = 1000\n"
     ]
    }
   ],
   "source": [
    "for node in inferred_model.graph.input:\n",
    "    names = node.name.split('/')\n",
    "    node_name = node.name\n",
    "    dim_values = [dim.dim_value for dim in node.type.tensor_type.shape.dim if dim.dim_value is not None]\n",
    "    # Print the size of each node\n",
    "    activations = 1\n",
    "    for dim in dim_values:\n",
    "        activations *= dim\n",
    "    print(f\"'{names[-1]}': {dim_values}, activations = {activations}\")\n",
    "    \n",
    "for node in inferred_model.graph.value_info:\n",
    "    names = node.name.split('/')\n",
    "    node_name = node.name\n",
    "    dim_values = [dim.dim_value for dim in node.type.tensor_type.shape.dim if dim.dim_value is not None]\n",
    "    # Print the size of each node\n",
    "    activations = 1\n",
    "    for dim in dim_values:\n",
    "        activations *= dim\n",
    "    print(f\"'{names[-1]}': {dim_values}, activations = {activations}\")\n",
    "\n",
    "for node in inferred_model.graph.output:\n",
    "    names = node.name.split('/')\n",
    "    node_name = node.name\n",
    "    dim_values = [dim.dim_value for dim in node.type.tensor_type.shape.dim if dim.dim_value is not None]\n",
    "    # Print the size of each node\n",
    "    activations = 1\n",
    "    for dim in dim_values:\n",
    "        activations *= dim\n",
    "    print(f\"'{names[-1]}': {dim_values}, activations = {activations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adda078-0ee0-417f-84d0-461964649649",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
